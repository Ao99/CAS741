\documentclass[12pt, titlepage]{article}

\input{../../Comments}
\input{../../Common}

\begin{document}

\title{Project Title: System Verification and Validation Plan for \progname{}} 
\author{Ao Dong}
\date{\today}
	
\maketitle

\pagenumbering{roman}

\section{Revision History}

\begin{tabularx}{\textwidth}{p{3cm}p{2cm}X}
\toprule {\bf Date} & {\bf Version} & {\bf Notes}\\
\midrule
Oct 27 & 1.0 & Initial Draft\\
Nov 11 & 1.1 & Minor Revision\\
\bottomrule
\end{tabularx}

\newpage

\tableofcontents

\listoftables

\listoffigures

\newpage

\section{Symbols, Abbreviations and Acronyms}

\renewcommand{\arraystretch}{1.2}
\begin{tabular}{l l} 
  \toprule		
  \textbf{symbol} & \textbf{description}\\
  \midrule 
  2D & Two-Dimensional\\
  DICOM & Digital Imaging and Communications in Medicine\\
  Git & \begin{tabular}[c]{@{}l@{}}a distributed version-control system for
          tracking changes in source code during\\ software
          development\end{tabular}\\
  IDE & Integrated Development Environment\\
  JIRA & a proprietary issue tracking product\\ 
  JUnit & a unit testing framework for the Java programming language.\\
  $k^{\star}$  & optimal  threshold  value found by Otsu' Method\\
$k^{\star}_{1}$ & optimal threshold value found by Otsu' Method with multiple
thresholds\\
$k^{\star}_{2}$ & optimal threshold value found by Otsu' Method with multiple
thresholds\\
  $L$ & number  of  the  discrete  levels  of  the  feature value\\
  MG & Module Guide\\
  \famname{} & Medical Imaging Applications\\
  MIS & Module Interface Specification\\
  \progname{} & Medical Imaging Segmentation Library\\
Redmine & a free and open source, web-based project management and issue
tracking tool\\
  SourceForge & \begin{tabular}[c]{@{}l@{}}a web-based service that offers
software developers a centralized online location\\ to control and manage free
and open-source software
                  projects\end{tabular}\\
  SRS & Software Requirements\\
  T & Test\\
  Trac & an open-source, Web-based project management and bug tracking system\\
  VnV & Verification and Validation\\
  \bottomrule
\end{tabular}\\


\newpage

\pagenumbering{arabic}

This document describes procedures concerning the testing of one software of
theMIA family for compliance with the requirements. It also describes how the
quality of the program is assured.

Some general information such as introduction to the software and testing
objectives are included in Section \ref{sec_geinfo}. Verification plans and
testdescriptions are in Section \ref{sec_plan} and \ref{sec_systestdescri},
respectively.

\section{General Information} \label{sec_geinfo}

\subsection{Summary}

The software going through the test is Medical Imaging Segmentation
(\progname{}).

Segmentation, separation of structures of interest from the background and from
each other~\cite{Bankman2000}. Image segmentation is the process of
partitioningan image into different meaningful segments. In medical imaging,
these segments
often correspond to different tissue classes, organs, pathologies, or other
biologically relevant structures~\cite{Forouzanfar2010}.

\progname{} uses one of many segmentation algorithms - the Intensity Threshold
method. It also uses Otsu's Method to find the optimal threshold value(s).
Afterreceiving input medical image from the users, \progname{} calculates the
optimalthreshold value(s), and output the processed segmentation image.

\subsection{Objectives}
\label{Sec_objectives}
The requirements that the software has to be verified against can be found in
the SRS document. All the functional and nonfunctional requirements should be
tested, with test descriptions in Section \ref{sec_systestdescri}.

The goal of verifying and validating is to increase confidence in the software
implementation. The most important qualities to focus on are nonfunctional
requirements, such as correctness and usability.

This document will be used as a starting point for the verification and
validation report.  The test cases presented within this document will be
executed and the output will be analyzed to determine if the software is
implemented correctly.

\subsection{Relevant Documentation}

\begin{itemize}
    \item SRS \cite{Dong2019SRS}
    \item Unit VnV Plan \cite{Dong2019UnitVnv}
    \item MG \cite{Dong2019MG}
    \item MIS \cite{Dong2019MIS}
\end{itemize}

\wss{Your design documents are also relevant. You can ``fake it'' and list them
here too.}
\an{I've cited all relevant documents here}

\section{Plan} \label{sec_plan}

The following sections provide more detail about the VnV of the
\progname{} family. Information about the testing participants is provided, and
the verification plans for SRS, design, implementation and validation plan for
software are described.

\subsection{Verification and Validation Team}

\begin{itemize}
    \item Ao Dong
    \item Prof. Spencer Smith
    \item Peter Michalski
    \item Zhi Zhang
    \item Sasha Soraine
    \item Sharon Wu
    
\wss{You can be more specific here. Some students have explicitly been assigned
to work on your project. You should also list the course instructor as part of
the VnV team.}
\an{added relevant individuals' names here}

\end{itemize}

\subsection{SRS Verification Plan}

The SRS can be reviewed with the help from the author's professor and
classmates. Teamwork will be done systematically by reviewing each others SRS
orother documents. The whole process can be done through GitHub by reviewing
andsubmitting issues. Reviewers can give revision suggestions to the author, and
the author has the responsibility to check all the submitted issues and make
necessary adjustments accordingly.

\subsection{Design Verification Plan}

During the writing of SRS, the identification of verification activity is
considered parallel. This enables the writer to make sure that the
specificationin the SRS is verifiable. Any future changes to the SRS should not
compromise
the verifiability. \wss{This last
  sentence is not a complete thought.}\an{adjusted}

Some details need to be identified, such as measurement methods, test
environment, development strategy, resources, tools, and facilities. Before
making the final plan, the proposed plan can be reviewed by the VnV team, and
issues can be submitted to improve the plan.

Usually the plan should be ready before the implementation stage. However,
during the implementation, if specifications need to be modified in SRS, the
plan
might need to be updated accordingly.

The specifications and test plan shall be well-documented. There can be
preliminary test plan to make improvements to the final plan.

\wss{This is not a very specific plan. Who is going to review your design?
Howare they going to do the review? Peter had some specific ideas for doing
thisthat you might like to borrow.}

\an{Peter's paper only has two sentences here. I borrowed the following idea
anyway.}

The verification of the design shall help achieving the objectives described in
Section \ref{Sec_objectives} \cite{Michalski2019SystVnv}. Specifically,
the verification should ensure that all the functional requirements and
nonfuncionaly requirements can be fully tested.

\an{I borrowed Sasha's idea as follows.}

Three methods can be used for the design verification
\cite{Soraine2019SystVnv}:\begin{itemize}
\item Rubber duck testing. This verification shall be done by the author. It
includes examining the whole MG and MIS and explaining the software process
aloud to a (imaginary) rubber duck.

\item Expert review. This verification shall be done by Prof.\ Smith and the MG
and MIS reviewers Zhi Zhang and Sharon Wu. The design shall be verified to be
able to meet all the requirements in the SRS. The reviewing process shall use
anissue tracking and version control tool such as GitHub.

\item Task-based peer review. This verification shall be done by Peter
Michalskiand other interested peers, who can complete individual tasks by
verifying
certain parts of the whole software design.
\end{itemize}

\subsection{Implementation Verification Plan}

Specific verification methods will be carefully chosen for functional and
nonfunctional requirements respectively. For instance, both Static Verification
and Dynamic Verification will be used.
    
    Static aspects such as code conventions, software metrics calculation,
anti-pattern detection will be analyzed for some nonfunctional requirements in
Section \ref{sec_nonfuncreqtest}. Both manual and automatic techniques will be
used for investigation, mathematical calculations, logical evaluation,
etc. Regarding the automatic techniques, some static code analyzer can be used,
such as PMD (\url{https://pmd.github.io/}), \wss{Have a look at the writing
checklist - additional spaces are needed before the opening brackets.}
\an{adjusted}
LGTM (\url{https://lgtm.com/help/lgtm/about-lgtm}) and Deep Dive
(\url{https://discotek.ca/deepdive.xhtml}).
    
    After selecting a group of test cases consisting of tests data, dynamic
verification will be used by execution of the system or its units. By finding
out the output test results, we can execute testings for the functional
requirements in Section \ref{sec_funcreqtest}. By methods like questionnaire
andinterview, we can analyze the nonfunctional requirements listed in Section
\ref{sec_nonfuncreqtest}.

\subsection{Software Validation Plan}

One possible validation approach is interviewing Dr.\ Michael Noseworthy to
findout if \progname{} is really what the users need.

\section{System Test Description} \label{sec_systestdescri}

\subsection{Tests for Functional Requirements} \label{sec_funcreqtest}

The functional requirements described in the SRS \cite{Dong2019SRS}. \wss{It is
a
  maintenance nightmare to copy and paste between documents.  You can just
  reference the original document.  Using the original files in the Blank
  Project Template repo with make, it is possible to cross-reference between
  documents.}
\an{I cited SRS here, and deleted the copied words}

\progname{} shall verify that the input data are valid, shall guarantee that
theoutput is consistent with the input and meet the same standard, and shall
provide correct calculation and output.

R1 will be tested in Section \ref{sec_inputtest}, R3 will be tested in
\ref{sec_caltest}, R5 in Section \ref{sec_outputtest} and R2 and R4 in Section
\ref{sec_outputverifytest}.

\subsubsection{Input verification}
\label{sec_inputtest}

According to R1 in the SRS, \progname{} shall verify that the input data are
valid. A valid input image must be 12-bit or 16-bit grayscale DICOM image. An
error message shall be displayed if input data are invalid.

Part of the test for nonfunctional requirements including Robustness in Section
\ref{sec_robustnesstest} is also done here.

In this test, various types of input file will be tested, \progname{} shall
onlytake 12-bit or 16-bit grayscale DICOM image with width and height greater
than 0
and with valid pixel values as input, as described in the Section Input Data
Constraints in the SRS. Taken incorrect format or
file type, it shall display an error message.
		
\paragraph{Input Verification Test}

\begin{enumerate}

\item{Invalid filename extensions}

Control: Automatic
					
Initial State: \progname{} is started and running
					
Input: the prepared files invalid.txt, invalid.pdf, invalid.jpg in the same
folder as this document; or a random file with an invalid filename extension,
such as .txt, .pdf and .jpg, which shall not be .dcm nor .dcm30.
					
Output: \progname{} shall display warning that this file is not supported.

Test Case Derivation: successfully display error message.
					
How test will be performed: it will be performed by the test team with help of
JUnit. The test team should load the whole project with an IDE and run the
JUnittest file /test/InputFilenameTest.java. \wss{You should make this test
automatic.
  Using a unit testing framework, a test case can be defined that is considered
  successful is the correct exception is raised.}
\an{Modified to use a ``fake" JUnit test module which might won't be in the
folder soon}

					
\item{Invalid file format}

Control: Automatic
					
Initial State: \progname{} is started and running
					
Input: the prepared file invalid.dcm and invalid.dcm30 in the same folder as
this document; or a random file whose filename extension has been changed from
an invalid one to .dcm and .dcm30, but the file format is not 12-bit or 16-bit
DICOM image.
					
Output: \progname{} shall display warning that this file might be damaged or
theformat is not supported.

Test Case Derivation: successfully detect the data format in the file and
display error message. 

How test will be performed: it will be performed by the test team with help of
JUnit. The test team should load the whole project with an IDE and run the
JUnittest file /test/InputFormatTest.java. \wss{Again, this can be automated
using a
unit
  testing framework.  It can again be handled with exceptions.}
\an{Modified}

\item{Valid input file}

Control: Manual
					
Initial State: \progname{} is started and running
					
Input: the prepared file valid.dcm and valid.dcm30 in the same folder as this
document; or a file with an valid filename extension, such as .dcm and .dcm30,
and the file format is 12-bit or 16-bit DICOM image.
					
Output: \progname{} shall allow this file as an input, and display a success
message.

Test Case Derivation: successfully accept the valid file and display a message.

How test will be performed: it will be performed by the test team manually, and
will be repeated multiple times.
\end{enumerate}

\subsubsection{Calculation}
\label{sec_caltest} \progname{} shall provide correct calculate according to
Instance Models according to the user's choice of which method to use, single
ormultiple global thresholds. \progname{} shall also display the correct
optimalthreshold value(s) $k^{\star}$ or $k^{\star}_{1}$ and $k^{\star}_{2}$
accordingly.

Part of the test for nonfunctional requirements including Correctness and
Verifiability in Section \ref{sec_correctverfiabletest} is also done here.

In this test, calculated values will be cross-checked with results from
ITK-SNAPMedical Image Segmentation Tool
(\url{https://sourceforge.net/projects/itk-snap/}). \wss{I don't like the
``suchas'' here. Now is the time
  to make specific decisions.  If you are going to use VTK say that, if you are
  going to use something else, say that.  Being specific here does not mean you
are tied to the decision, you could always change your mind later and ``fake''
the documentation.}
\an{Changed it to a specific software}
\paragraph{Calculation Test}

\begin{enumerate}

\item{Display single threshold value}

Control: Automatic
					
Initial State: \progname{} is started and running, /test/sample\_image.dcm has
been taken as the input. \wss{I would like you to be more specific.
  What it the image?  Where can I find it?  A sample image from the VTK
  documentation would be fine, but you need to make a specific decision.  Also,
  I believe that the input image is part of input, not the initial state.  This
  same comment about identifying the specific image occurs throughout your test
  cases.  It would be fine if you used the same image for multiple tests.}
\an{Changed it to an specific input image which is also ``fake", it won't
actually be in the folder soon.}
\an{I prefer to remain the input part here because this test assume that an
input is successfully taken. If any errors happen during the input part, they
should be outside the scope of this test.}

Input: user shall choose the first one from the two options: Single or Multiple
Global Thresholds.
					
Output: accordingly, \progname{} shall calculate and display one optimal
threshold value $k^{\star}$, where $k^{\star} \in \{1, 2, 3, ..., L-2\}$. The
value will be compared with output from VTK to show the correctness.

Test Case Derivation: successfully detect the users' choice and display
potentially correct number of values.

How test will be performed: the same input image will be used for calculation
inVTK, and the output optimal threshold values from VTK will be used as control
value. The percentage of difference between the outputs from \progname{} and
VTKwill be calculated. It will be performed by the test team with assistance
from
JUnit, and will be repeated multiple times.

\item{Display double threshold values}

Control: Automatic
					
Initial State: \progname{} is started and running, /test/sample\_image.dcm has
been taken as the input.
					
Input: user shall choose the second one from the two options: Single or
MultipleGlobal Thresholds.
					
Output: accordingly, \progname{} shall calculate and display two optimal
threshold values - $k^{\star}_{1}$ and $k^{\star}_{2}$, where $k^{\star}_{1}
\in[1, k^{\star}_{2}-2]$ and $k^{\star}_{2} \in [k^{\star}_{1}+2,L-2]$. The
values
will be compared with output from VTK to show the correctness.

Test Case Derivation: successfully detect the users' choice and display
potentially correct number of values.

How test will be performed: the same input image will be used for calculation
inVTK, and the output optimal threshold values from VTK will be used as control
value. The percentage of difference between the outputs from \progname{} and
VTKwill be calculated. It will be performed by the test team with assistance
from
JUnit, and will be repeated multiple times.

\end{enumerate}

\subsubsection{Output}
\label{sec_outputtest}

\progname{} shall output segmentation image. In this test, given a valid input,
an output image is expected.
		
\paragraph{Output Test}

\begin{enumerate}

\item{Existence of output file}

Control: Automatic
					
Initial State: \progname{} is started and running, a valid input image such as
valid.dcm or valid.dcm30 is taken, user has chosen which threshold method to
use, optimal threshold value(s) have been displayed.
					
Input: User shall start the next step.
					
Output: \progname{} shall output a file, and this output file shall be found
with correct filename extension .bmp.

Test Case Derivation: successfully detect the users' choice and provide an
output file with valid filename extension.

How test will be performed: it will be performed by the test team with help of
JUnit. The test team should load the whole project with an IDE and run the
JUnittest file /test/OutputFileWriteTest.java. \wss{This can be automated.}
\an{adjusted}

\end{enumerate}

\subsubsection{Output verification} \label{sec_outputverifytest}

\progname{} shall guarantee that the output file is the same resolution as the
input file, and shall verify that the output data are valid and meet the format
standards. The output image must be 2D 8-bit grayscale image and the pixel
format must be the byte image, where the feature value must be the gray
intensity value stored as an 8-bit integer giving a range of possible values
from 0 to 255.

Part of the test for nonfunctional requirements including Correctness and
Verifiability in Section \ref{sec_correctverfiabletest} is also done here.

In this test, output image will be cross-checked with results from other
software such as VTK.
		
\paragraph{Output verification test}

\begin{enumerate}

\item{Valid file format of output file}

Control: Automatic
					
Initial State: \progname{} is started and running, a valid input image a valid
input image such as valid.dcm or valid.dcm30 is taken, user has chosen which
threshold method to use, optimal threshold value(s) have been displayed, a file
has been output.
					
Input: the input and output file.
					
Output: the result of whether the output image is an 8-bit grayscale image with
the same resolution as the input file.

Test Case Derivation: successfully output file with correct filename
extensions.How test will be performed: it will be performed by the test team
with the help
of JUnit and OpenCV Java library. The test team should load the whole project
with an IDE and run the JUnit test file /test/OutputFormatTest.java, which is
built using OpenCV Java library for image reading and JUnit for testing.
\wss{This test can also be automated. That should be your goal. I don't
  think it will be that difficult if you use the right library.  (Steven
  Palmer's chemical speciation example from one of the recent years used a
  library to do this.)}
\an{adjusted}

\item{Correctness of output file}

\an{What documented here is the very trivial way of comparing the output with
the control image. More sophisticated and accurate method should be add to here
later}

Control: Automatic
					
Initial State: \progname{} is started and running, a valid input image a valid
input image such as valid.dcm or valid.dcm30 is taken, user has chosen which
threshold method to use, optimal threshold value(s) have been displayed, a file
has been output.
					
Input: the output file.
					
Output: the percentage of pixel value difference between this file and the
output file from VTK using the same threshold value(s).

Test Case Derivation: successfully compare the output file with control image
from VTK.

How test will be performed: it will be performed by the test team with the help
of JUnit and OpenCV Java library. The test team should load the whole project
with an IDE and run the JUnit test file /test/OutputCorrectnessTest.java, which
is built using OpenCV Java library for image reading and JUnit for testing. The
resolution and pixel values of input and output images will be compared, and
thedifference will be measured.

\end{enumerate}

\subsection{Tests for Nonfunctional Requirements} \label{sec_nonfuncreqtest}

The functional requirements described in the SRS \cite{Dong2019SRS}.

\wss{Really just referencing the section is enough, since
  reproducing the text creates a maintainability problem.}
\an{I cited SRS here, and deleted the copied words}

All the qualities of \progname{} will be tested in the following 
subsections. Most qualities can be measured by the grade sheet in tables, such
as Table \ref{Tb_install} for Installability. In some cases a superscript $*$
isused to indicate that a response of this type should be accompanied by
explanatory text. For instance, if problems were caused by uninstall, the
reviewer should note what problems were caused. An (I) precedes the test case
orquestion description when its measurement requires a successful installation
\cite{SmithEtAl2018}.

\subsubsection{Installability}
\label{sec_installtest} Installability is the degree of effectiveness and
efficiency with which a product or system can be successfully installed and/or
uninstalled in a specified environment \cite{ISO/IEC25010:2011}.
	
\paragraph{Installability test}

\begin{enumerate}

\item{Installation and uninstallation on Windows system}
\label{sec_InstallWindows}

Type: Manual
					
Initial State: a virtual machine of fresh Windows 10 operating system
					
Input/Condition: \progname{} installation package, install command and
uninstallcommand after installation
					
Output/Result: the degree of effectiveness and efficiency with which
\progname{}can be successfully installed and/or uninstalled
					
How test will be performed: Installability can be measured by the grade sheet
inTable \ref{Tb_install}. It will be performed by the test team manually.

\begin{table}[h]
\begin{tabular}{@{}ll@{}}
\toprule
Questions & Sets of Answers \\ \midrule
Are there installation instructions? & \{yes,no\} \\
Are the installation instructions linear? & \{yes, no, N/A\} \\
Is there something in place to automate the installation? & \{yes*, no\} \\
Is there a means given to validate the installation? & \{yes*, no\} \\
How many steps were involved in the installation? & $\mathbb{N}$ \\
How many software packages need to be installed? & $\mathbb{N}$ \\
Run uninstall, if available. Any obvious problems? & \{yes*, no, n/a\} \\
Overall Impression & \{1 .. 10\}\\ \bottomrule
\end{tabular}
\caption{Installability Grade Sheet~\cite{SmithEtAl2018}}
\label{Tb_install}
\end{table}

\item{Installation and uninstallation on Mac system}

Type: Manual
					
Initial State: a virtual machine of fresh macOS 10.14 operating system
					
Input/Condition: \progname{} installation package, install command and
uninstallcommand after installation
					
Output/Result: the degree of effectiveness and efficiency with which
\progname{}can be successfully installed and/or uninstalled
					
How test will be performed: same as Installation and uninstallation on Windows
system.

\item{Installation and uninstallation on Linux system}

Type: Manual
					
Initial State: a virtual machine of fresh Ubuntu Linux 18.04 operating system
					
Input/Condition: \progname{} installation package, install command and
uninstallcommand after installation
					
Output/Result: the degree of effectiveness and efficiency with which
\progname{}can be successfully installed and/or uninstalled
					
How test will be performed: same as Installation and uninstallation on Windows
system.
\end{enumerate}

\wss{I agree that installability needs to be done with manual tests.}

\subsubsection{Correctness and Verifiability} \label{sec_correctverfiabletest}

The term correctness is often mentioned as a degree to which software meets the
requirement specification~\cite{IEEE1990}.  Verifiability is sometimes referred
to as testability, since the focus is on measuring how easily the properties of
a software can be checked or proven~\cite{SmithEtAl2018}.
		
Correctness is tested in the tests for functional
requirements included in Section \ref{sec_caltest} and
\ref{sec_outputverifytest}. \wss{Yes I agree with assessing correctness, but
youaren't really measuring verifiability. Your tests could be very difficult to
  perform.  Just running tests just not prove that the software is verifiable.}
\an{Added more contents as follows for verifiability}

Correctness and verifiability will also be measured by the grade sheet in Table
\ref{Tb_correctnessVerifiability}. It will be performed by the test team
manually.

\begin{table}[h]
\begin{tabular}{@{}ll@{}}
\toprule
Questions & Sets of Answers \\ \midrule
Are external libraries used? & \{yes*, no, unclear\}\\
Does the community have confidence in this library? & \{yes, no, unclear\} \\
Any reference to the requirements specifications of the program? & \{yes*, no,
unclear\} \\
What tools or techniques are used to build confidence of correctness? & string
\\
(I) If there is a getting started tutorial, is the output as expected? & \{yes,
no*, n/a\} \\
Overall impression? & \{1 .. 10\} \\ \bottomrule
\end{tabular}
\caption{Correctness and Verifiability Grade Sheet~\cite{SmithEtAl2018}}
\label{Tb_correctnessVerifiability}
\end{table}

\subsubsection{Robustness} \label{sec_robustnesstest}

A program is robust if it behaves ``reasonably'', even
in circumstances that were not anticipated in the requirements specification -
for example, when it encounters incorrect input data or some hardware
malfunction~\cite{Ghezzi1991}.

It will be measured by the grade sheet in Table \ref{Tb_robustness}. It will be
performed by the test team manually.

\begin{table}[h]
\begin{tabular}{@{}ll@{}}
\toprule
Questions & Sets of Answers \\ \midrule
(I) Does the software handle garbage input reasonably? & \{yes, no*\}\\
\begin{tabular}[c]{@{}l@{}}(I) For any plain text input files, if all new lines
are replaced with new lines\\
and carriage returns, will the software handle this gracefully? \end{tabular} &
\{yes, no*, n/a\} \\
Overall impression? & \{1 .. 10\} \\ \bottomrule
\end{tabular}
\caption{Robustness Grade Sheet~\cite{SmithEtAl2018}}
\label{Tb_robustness}
\end{table}

\wss{Technically, this isn't robustness testing if your
  requirements explicitly say what to do in these input error cases.  I cannot
  remember whether your requirements are specific about these scenarios.}
\an{I changed it to your questionnaire. Before finding an better method, I
thinkit's the best way of measuring.}

\subsubsection{Usability} \label{sec_usabilitytest}

Usability is the degree to which a product or system
can be used by specified users to achieve specified goals with effectiveness,
efficiency and satisfaction in a specified context of use
\cite{ISO/IEC25010:2011}.
    
    It will be measured by 4 qualities: Learnability, Memorability, Efficiency
and Satisfaction.
		
\paragraph{Usability Test}

\begin{enumerate}

\item{Learnability}

Type: Manual
					
Initial State: \progname{} is started and running.
					
Input/Condition: A new user to \progname{} is asked to learn the software by
himself/herself and to accomplish the task of inputting image, choosing the
multiple-threshold
calculation method and outputting image. If user guide exists, it shall be
provided. \wss{I like this, but I think you can be more specific.  What
  calculation method are they asked to choose?}
\an{I specified to choose from one of the methods}
				
Output/Result: time to completion, number of misoperations and percentage of
success will be measured.
					
How test will be performed: results will be recorded in the grade sheet in
Table\ref{Tb_use}. It will be performed by the test team manually.

\item{Memorability}

Type: Manual
					
Initial State: \progname{} is started and running.
					
Input/Condition: 2 weeks after the new user finish the Learnability test, he or
she shall be asked to accomplish the same tasks again. No guide shall be
provided.
					
Output/Result: time to completion, number of misoperations and percentage of
success will be measured, and percentage of improvements will be calculated
					
How test will be performed: results will be recorded in the grade sheet in
Table\ref{Tb_use}. It will be performed by the test team manually.
					
\item{Efficiency}

Type: Manual
					
Initial State: \progname{} is started and running.
					
Input/Condition: a proficient user to \progname{} is asked to accomplish the
task of inputting image, choosing calculation method and outputting image. No
guide shall be provided.
					
Output/Result: time to completion and number of misoperations will be measured
					
How test will be performed: results will be recorded in the grade sheet in
Table\ref{Tb_use}. It will be performed by the test team manually.
					
\item{Satisfaction}

Type: Manual
					
Initial State: \progname{} is started and running.
					
Input/Condition: a user to \progname{} is asked to answer additional questions
and provide a overall satisfaction grade to the software
					
Output/Result: answer from the user
					
How test will be performed: results will be recorded in the grade sheet in
Table\ref{Tb_use}. It will be performed by the test team manually.

\end{enumerate}

\subsubsection{Maintainability}
\label{sec_Maintaintest} Maintainability is the degree of effectiveness and
efficiency with which a product or system can be modified by the intended
maintainers~\cite{ISO/IEC25010:2011}.

\paragraph{Maintainability Test}

\begin{enumerate}

\item{Development process check}

Type: Manual
					
Initial State: N/A
					
Input/Condition: Testers need to review the whole development process, and
answer questions related to the ease of maintainability.
					
Output/Result: answers and grades to the table.
					
How test will be performed: testers shall check the GitHub repo of \progname{}
for the effectiveness of version control and issue tracking on the software and
the documents; they shall check the existence and completeness of the documents
such as SRS, SysVnVPlan, MG, MIS. Results will be recorded in the grade sheet
inTable \ref{Tb_maintain}. It will be performed by the test team manually.
\end{enumerate}

\begin{table}[]
\begin{tabular}{ll} \hline Questions & Sets of Answers \\ \hline Is there a
history of multiple versions of the software? & \{yes, no, unclear\} \\
\begin{tabular}[c]{@{}l@{}}Is there any information on how code is reviewed,
or\\ how to contribute?\end{tabular} & \{yes*, no\} \\ Is there a changelog? &
\{yes, no\} \\ What is the maintenance type? & \{corrective, adaptive,
perfective, unclear\} \\ What issue tracking tool is employed?
& \begin{tabular}[c]{@{}l@{}}\{Trac, JIRA, Redmine, e-mail, discussion board,
SourceForge,\\Git, none, unclear\}\end{tabular} \\ Are the majority of
identified bugs fixed? & \{yes, no*, unclear\} \\ Which version control system
is in use? & \begin{tabular}[c]{@{}l@{}}\{svn, cvs, git,\\ github,
unclear\}\end{tabular} \\
\begin{tabular}[c]{@{}l@{}}Is there evidence that maintainability was
considered\\ in the design?\end{tabular} & \{yes*, no\} \\ Are there code
clones? & \{yes*, no, unclear\} \\ Overall impression? & \{1 .. 10\} \\ \hline
\end{tabular}
\caption{Maintainability Grade Sheet~\cite{SmithEtAl2018}}
\label{Tb_maintain}
\end{table}

\subsubsection{Portability}
\label{sec_portabletest} Portability is the degree of effectiveness and
efficiency with which a system, product or component can be transferred from
onehardware, software or other operational or usage environment to
another~\cite{ISO/IEC25010:2011}.

\paragraph{Portability Test}

\begin{enumerate}

\item{Portability on Windows system}

Type: Manual
					
Initial State: \progname{} has been successfully installed on a virtual machine
of fresh Windows 10 operating system
					
Input/Condition: operate the basic functions of the software
					
Output/Result: the degree of effectiveness and efficiency with which
\progname{}can be operate on this platform
					
How test will be performed: Portability can be measured by the grade sheet in
Table \ref{Tb_portable}. It will be performed by the test team manually.

\begin{table}[h]
\begin{tabular}{@{}ll@{}}
\toprule
Questions & Sets of Answers \\ \midrule
(I)What platforms is the software advertised to work on?
& \begin{tabular}[c]{@{}l@{}}\{Windows, Linux, macOS,\\ Android, Other
OS\}\end{tabular} \\
\begin{tabular}[c]{@{}l@{}}(I)Is there any compromise to functional or
  nonfunctional\\ requirements by running on this platform?\end{tabular} &
\{yes*, no\} \\Are special steps taken in the source code to handle portability?& \{yes*, no,
n/a\} \\
Is portability explicitly identified as NOT being important? & \{yes, no\} \\
Convincing evidence that portability has been achieved? & \{yes*, no\} \\
Overall impression? & \{1 .. 10\} \\ \bottomrule
\end{tabular}
\caption{Portability Grade Sheet~\cite{SmithEtAl2018}}
\label{Tb_portable}
\end{table}

\item{Portability on Mac system}

Type: Manual
					
Initial State: \progname{} has been successfully installed on a virtual machine
of fresh macOS 10.14 operating system
					
Input/Condition: operate the basic functions of the software
					
Output/Result: the degree of effectiveness and efficiency
with which \progname{} can be operate on this platform
					
How test will be performed: Portability can be measured by the grade sheet in
Table \ref{Tb_portable}. It will be performed by the test team manually.

\item{Portability on Linux system}

Type: Manual
					
Initial State: \progname{} has been successfully installed on a virtual machine
of fresh Ubuntu Linux 18.04 operating system
					
Input/Condition: operate the basic functions of the software
					
Output/Result: the degree of effectiveness and efficiency
with which \progname{} can be operate on this platform
					
How test will be performed: Portability can be measured by the grade sheet in
Table \ref{Tb_portable}. It will be performed by the test team manually.
\end{enumerate}

\subsubsection{Understandability}
\label{sec_understandtest}

Understandability measures the ease with which a new developer can understand
the design and source code. Good understandability contributes to
maintainability, and provides critical information for
verifiability~\cite{SmithEtAl2018}.

\paragraph{Understandability Test}

\begin{enumerate}

\item{Code review}

Type: Manual
					
Initial State: the development of \progname{} is completed and the source code
is accessible
					
Input/Condition: review the source code
					
Output/Result: the ease with which a new developer can understand the source
code
					
How test will be performed: After reading part or all of the source code,
Understandability can be measured by the grade sheet in Table
\ref{Tb_understandable}. It will be performed by the test team manually.

% Please add the following required packages to your document preamble:
% \usepackage{booktabs}
\begin{table}[h]
\begin{tabular}{@{}ll@{}}
\toprule
Questions & Set of Answers \\ \midrule
Consistent indentation and formatting style? & \{yes, no, n/a\} \\
Explicit identification of a coding standard? & \{yes*, no, n/a\} \\
Are the code identifiers consistent, distinctive, and meaningful? & \{yes, no*,
n/a\} \\
Are constants (other than 0 and 1) hard-coded into the program? & \{yes, no*,
n/a\} \\
Comments are clear, indicate what is being done, not how? & \{yes, no*, n/a\}
\\Is the name/URL of any algorithms used mentioned? & \{yes, no*, n/a\} \\
Parameters are in the same order for all functions? & \{yes, no*, n/a\} \\
Is code modularized? & \{yes, no*, n/a\} \\
Descriptive names of source code files? & \{yes, no*, n/a\} \\
Is a design document provided? & \{yes*, no, n/a\} \\
Overall impression? & \{1 .. 10\} \\ \bottomrule
\end{tabular}
\caption{Understandability Grade Sheet~\cite{SmithEtAl2018}}
\label{Tb_understandable}
\end{table}

\item{MG and MIS review}

Type: Manual
					
Initial State: the development of \progname{} is completed and the MG and MIS
documents are accessible
					
Input/Condition: review the MG and MIS documents
					
Output/Result: the ease
with which a new developer can understand the design
					
How test will be performed: After reading the MG and MIS, Understandability can
be measured by the grade sheet in Table \ref{Tb_understandable}. it will be
performed by the test team manually.

\end{enumerate}

\wss{Good work with the nonfunctional requirements.  I hope you are able to do
  at least some of these measurements.  I think we would learn from collecting
  the data, and this could be helpful for your MEng project.}

\subsection{Traceability Between Test Cases and Requirements}

The purpose of the traceability matrices is to provide easy references on what
has to be additionally modified if a certain component is changed. Every time a
component is changed,the items in the column of that component that are marked
with an “X” may have to be modified as well. Table \ref{Tb_trace} shows the
dependencies between the test cases and the requirements.
\wss{Include some text to introduce the table.}
\an{Added text similar to the SRS}

\begin{table}[h]
\begin{tabular}{|l|l|l|l|l|l|l|l|l|l|l|l|l|l|}
\hline
 & R1 & R2 & R3 & R4 & R5 & R6 & R7 & R8 & R9 & R10 & R11 & R12 & R13 \\ \hline
\ref{sec_inputtest} & X &  &  &  &  &  &  &  & X &  &  &  &  \\ \hline
\ref{sec_caltest} &  &  & X &  &  &  & X & X &  &  &  &  &  \\ \hline
\ref{sec_outputtest} &  &  &  &  & X &  &  &  &  &  &  &  &  \\ \hline
\ref{sec_outputverifytest} &  & X &  & X &  &  & X & X &  &  &  &  &  \\ \hline
\ref{sec_installtest} &  &  &  &  &  & X &  &  &  &  &  &  &  \\ \hline
\ref{sec_correctverfiabletest} & & & & & & & X & X & & & & & \\ \hline
\ref{sec_robustnesstest} &  &  &  &  &  &  &  &  & X &  &  &  &  \\ \hline
\ref{sec_usabilitytest} &  &  &  &  &  &  &  &  &  & X &  &  &  \\ \hline
\ref{sec_Maintaintest} &  &  &  &  &  &  &  &  &  &  & X &  &  \\ \hline
\ref{sec_portabletest} &  &  &  &  &  &  &  &  &  &  &  & X &  \\ \hline
\ref{sec_understandtest} &  &  &  &  &  &  &  &  &  &  &  &  & X \\ \hline
\end{tabular}
\caption{Traceability Matrix showing the connections between requirements and
tests}
\label{Tb_trace}
\end{table}

\newpage
				
\bibliographystyle{plainnat}

\bibliography{../../../refs/References}

\newpage

\section{Appendix}

\subsection{Symbolic Parameters}

The definition of the test cases will call for SYMBOLIC\_CONSTANTS.
Their values are defined in this section for easy maintenance.

\subsection{Usability Grade Sheet}
This grade sheet is used for test in Section \ref{sec_usabilitytest}
\begin{table}[h]
\begin{tabular}{lll}
\hline
Test ID & Question/test detail & Anser/result \\ \hline
\multirow{3}{*}{\begin{tabular}[c]{@{}l@{}}(I)Learnability:\\ new
                  users\end{tabular}} & Time to completion & Seconds \\
 & Number of misoperations & $\mathbb{N}$ \\
 & Percentage of success & Percentage \\ \hline
\multirow{6}{*}{\begin{tabular}[c]{@{}l@{}}(I)Memorability\\ second-time
                  users\end{tabular}} & Time to completion & Seconds \\
 & Percentage of improvement & Percentage \\
 & Number of misoperations & $\mathbb{N}$ \\
 & Percentage of improvement & Percentage \\
 & Percentage of success & Percentage \\
 & Percentage of improvement & Percentage \\ \hline
\multirow{2}{*}{\begin{tabular}[c]{@{}l@{}}(I)Efficiency:\\ proficient
                  users\end{tabular}} & Time to completion & Seconds \\
 & Number of misoperations & $\mathbb{N}$ \\ \hline
\multirow{9}{*}{\begin{tabular}[c]{@{}l@{}}(I)Satisfaction:\\ every
user\end{tabular}} & Do the operations fit to human nature and your intuition? &\{yes, no*\} \\
 & Does it support your language? & \{yes, no*\} \\
 & Can you understand the descriptions easily & \{yes, no*\} \\
 & Does it give a clear explanation when an error occurs? & \{yes, no*\} \\
 & Have you noticed any hot keys? & \{yes*, no\} \\
 & Do you think any hot key need to be added? & \{yes*, no\} \\
& Do you think undo or redo function is missing during any step? & \{yes*, no\}
\\
 & \begin{tabular}[c]{@{}l@{}}Do you think any other function for convenience
     need to be added?\\ Such as auto-fill, repeat and a record for all the
     steps.\end{tabular} & \{yes*, no\} \\
 & Overall satisfaction & \{1 .. 10\}\\
 \hline
\end{tabular}
\caption{Usability Grade Sheet}
\label{Tb_use}
\end{table}

\end{document}